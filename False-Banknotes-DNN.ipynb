{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ea243b8",
   "metadata": {},
   "source": [
    "# Project: Deep Neural Network\n",
    "- Identify false banknotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc0d19b",
   "metadata": {},
   "source": [
    "### Step 1: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5817679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b057d4",
   "metadata": {},
   "source": [
    "### Step 2: Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8993f69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.895690</td>\n",
       "      <td>3.00250</td>\n",
       "      <td>-3.606700</td>\n",
       "      <td>-3.44570</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.476900</td>\n",
       "      <td>-0.15314</td>\n",
       "      <td>2.530000</td>\n",
       "      <td>2.44950</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.910200</td>\n",
       "      <td>6.06500</td>\n",
       "      <td>-2.453400</td>\n",
       "      <td>-0.68234</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.607310</td>\n",
       "      <td>3.95440</td>\n",
       "      <td>-4.772000</td>\n",
       "      <td>-4.48530</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.371800</td>\n",
       "      <td>7.49080</td>\n",
       "      <td>0.015989</td>\n",
       "      <td>-1.74140</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>-2.967200</td>\n",
       "      <td>-13.28690</td>\n",
       "      <td>13.472700</td>\n",
       "      <td>-2.62710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>0.318030</td>\n",
       "      <td>-0.99326</td>\n",
       "      <td>1.094700</td>\n",
       "      <td>0.88619</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>-0.025314</td>\n",
       "      <td>-0.17383</td>\n",
       "      <td>-0.113390</td>\n",
       "      <td>1.21980</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>-2.234000</td>\n",
       "      <td>-7.03140</td>\n",
       "      <td>7.493600</td>\n",
       "      <td>0.61334</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>4.728500</td>\n",
       "      <td>2.10650</td>\n",
       "      <td>-0.283050</td>\n",
       "      <td>1.56250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1372 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      variance  skewness   curtosis  entropy  class\n",
       "0    -0.895690   3.00250  -3.606700 -3.44570      1\n",
       "1     3.476900  -0.15314   2.530000  2.44950      0\n",
       "2     3.910200   6.06500  -2.453400 -0.68234      0\n",
       "3     0.607310   3.95440  -4.772000 -4.48530      1\n",
       "4     2.371800   7.49080   0.015989 -1.74140      0\n",
       "...        ...       ...        ...      ...    ...\n",
       "1367 -2.967200 -13.28690  13.472700 -2.62710      1\n",
       "1368  0.318030  -0.99326   1.094700  0.88619      1\n",
       "1369 -0.025314  -0.17383  -0.113390  1.21980      1\n",
       "1370 -2.234000  -7.03140   7.493600  0.61334      1\n",
       "1371  4.728500   2.10650  -0.283050  1.56250      0\n",
       "\n",
       "[1372 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('banknotes.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74b029cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1372"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ed20c2",
   "metadata": {},
   "source": [
    "### Step 3: Investitigate the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a734a393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32701452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variance    0\n",
       "skewness    0\n",
       "curtosis    0\n",
       "entropy     0\n",
       "class       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9db20eb",
   "metadata": {},
   "source": [
    "### Step 4: Divite data into feature vectors and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31cb7d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.89569</td>\n",
       "      <td>3.00250</td>\n",
       "      <td>-3.606700</td>\n",
       "      <td>-3.44570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.47690</td>\n",
       "      <td>-0.15314</td>\n",
       "      <td>2.530000</td>\n",
       "      <td>2.44950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.91020</td>\n",
       "      <td>6.06500</td>\n",
       "      <td>-2.453400</td>\n",
       "      <td>-0.68234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.60731</td>\n",
       "      <td>3.95440</td>\n",
       "      <td>-4.772000</td>\n",
       "      <td>-4.48530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.37180</td>\n",
       "      <td>7.49080</td>\n",
       "      <td>0.015989</td>\n",
       "      <td>-1.74140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variance  skewness  curtosis  entropy\n",
       "0  -0.89569   3.00250 -3.606700 -3.44570\n",
       "1   3.47690  -0.15314  2.530000  2.44950\n",
       "2   3.91020   6.06500 -2.453400 -0.68234\n",
       "3   0.60731   3.95440 -4.772000 -4.48530\n",
       "4   2.37180   7.49080  0.015989 -1.74140"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dataset.iloc[:, :-1]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0ab73dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    0\n",
       "3    1\n",
       "4    0\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = dataset.iloc[:, -1]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c768c6c",
   "metadata": {},
   "source": [
    "### Step 5: Create training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fad22e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d80ba49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a809a69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(823, 549, 823, 549)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf02924",
   "metadata": {},
   "source": [
    "### Step 6: Create and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "590e7fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1b0b981",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(4,)))\n",
    "model.add(Dense(units=8, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6573ecdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Sequential in module keras.src.models.sequential:\n",
      "\n",
      "class Sequential(keras.src.models.model.Model)\n",
      " |  Sequential(*args, **kwargs)\n",
      " |  \n",
      " |  `Sequential` groups a linear stack of layers into a `Model`.\n",
      " |  \n",
      " |  Examples:\n",
      " |  \n",
      " |  ```python\n",
      " |  model = keras.Sequential()\n",
      " |  model.add(keras.Input(shape=(16,)))\n",
      " |  model.add(keras.layers.Dense(8))\n",
      " |  \n",
      " |  # Note that you can also omit the initial `Input`.\n",
      " |  # In that case the model doesn't have any weights until the first call\n",
      " |  # to a training/evaluation method (since it isn't yet built):\n",
      " |  model = keras.Sequential()\n",
      " |  model.add(keras.layers.Dense(8))\n",
      " |  model.add(keras.layers.Dense(4))\n",
      " |  # model.weights not created yet\n",
      " |  \n",
      " |  # Whereas if you specify an `Input`, the model gets built\n",
      " |  # continuously as you are adding layers:\n",
      " |  model = keras.Sequential()\n",
      " |  model.add(keras.Input(shape=(16,)))\n",
      " |  model.add(keras.layers.Dense(8))\n",
      " |  len(model.weights)  # Returns \"2\"\n",
      " |  \n",
      " |  # When using the delayed-build pattern (no input shape specified), you can\n",
      " |  # choose to manually build your model by calling\n",
      " |  # `build(batch_input_shape)`:\n",
      " |  model = keras.Sequential()\n",
      " |  model.add(keras.layers.Dense(8))\n",
      " |  model.add(keras.layers.Dense(4))\n",
      " |  model.build((None, 16))\n",
      " |  len(model.weights)  # Returns \"4\"\n",
      " |  \n",
      " |  # Note that when using the delayed-build pattern (no input shape specified),\n",
      " |  # the model gets built the first time you call `fit`, `eval`, or `predict`,\n",
      " |  # or the first time you call the model on some input data.\n",
      " |  model = keras.Sequential()\n",
      " |  model.add(keras.layers.Dense(8))\n",
      " |  model.add(keras.layers.Dense(1))\n",
      " |  model.compile(optimizer='sgd', loss='mse')\n",
      " |  # This builds the model for the first time:\n",
      " |  model.fit(x, y, batch_size=32, epochs=10)\n",
      " |  ```\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Sequential\n",
      " |      keras.src.models.model.Model\n",
      " |      keras.src.backend.tensorflow.trainer.TensorFlowTrainer\n",
      " |      keras.src.trainers.trainer.Trainer\n",
      " |      keras.src.layers.layer.Layer\n",
      " |      keras.src.backend.tensorflow.layer.TFLayer\n",
      " |      keras.src.backend.tensorflow.trackable.KerasAutoTrackable\n",
      " |      tensorflow.python.trackable.autotrackable.AutoTrackable\n",
      " |      tensorflow.python.trackable.base.Trackable\n",
      " |      keras.src.ops.operation.Operation\n",
      " |      keras.src.saving.keras_saveable.KerasSaveable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, layers=None, trainable=True, name=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  add(self, layer, rebuild=True)\n",
      " |      Adds a layer instance on top of the layer stack.\n",
      " |      \n",
      " |      Args:\n",
      " |          layer: layer instance.\n",
      " |  \n",
      " |  build(self, input_shape=None)\n",
      " |  \n",
      " |  call(self, inputs, training=None, mask=None)\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |  \n",
      " |  compute_output_spec(self, inputs, training=None, mask=None)\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the object.\n",
      " |      \n",
      " |      An object config is a Python dictionary (serializable)\n",
      " |      containing the information needed to re-instantiate it.\n",
      " |  \n",
      " |  pop(self, rebuild=True)\n",
      " |      Removes the last layer in the model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_config(config, custom_objects=None)\n",
      " |      Creates an operation from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`, capable of instantiating the\n",
      " |      same operation from the config dictionary.\n",
      " |      \n",
      " |      Note: If you override this method, you might receive a serialized dtype\n",
      " |      config, which is a `dict`. You can deserialize it as follows:\n",
      " |      \n",
      " |      ```python\n",
      " |      if \"dtype\" in config and isinstance(config[\"dtype\"], dict):\n",
      " |          policy = dtype_policies.deserialize(config[\"dtype\"])\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A Python dictionary, typically the output of `get_config`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An operation instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  input_dtype\n",
      " |      The dtype layer inputs should be converted to.\n",
      " |  \n",
      " |  input_shape\n",
      " |  \n",
      " |  inputs\n",
      " |  \n",
      " |  output_shape\n",
      " |  \n",
      " |  outputs\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  layers\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.models.model.Model:\n",
      " |  \n",
      " |  build_from_config(self, config)\n",
      " |      Builds the layer's states with the supplied config dict.\n",
      " |      \n",
      " |      By default, this method calls the `build(config[\"input_shape\"])` method,\n",
      " |      which creates weights based on the layer's input shape in the supplied\n",
      " |      config. If your config contains other information needed to load the\n",
      " |      layer's state, you should override this method.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: Dict containing the input shape associated with this layer.\n",
      " |  \n",
      " |  export(self, filepath, format='tf_saved_model', verbose=True)\n",
      " |      Create a TF SavedModel artifact for inference.\n",
      " |      \n",
      " |      **Note:** This can currently only be used with\n",
      " |      the TensorFlow or JAX backends.\n",
      " |      \n",
      " |      This method lets you export a model to a lightweight SavedModel artifact\n",
      " |      that contains the model's forward pass only (its `call()` method)\n",
      " |      and can be served via e.g. TF-Serving. The forward pass is registered\n",
      " |      under the name `serve()` (see example below).\n",
      " |      \n",
      " |      The original code of the model (including any custom layers you may\n",
      " |      have used) is *no longer* necessary to reload the artifact -- it is\n",
      " |      entirely standalone.\n",
      " |      \n",
      " |      Args:\n",
      " |          filepath: `str` or `pathlib.Path` object. Path where to save\n",
      " |              the artifact.\n",
      " |          verbose: whether to print all the variables of the exported model.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      # Create the artifact\n",
      " |      model.export(\"path/to/location\")\n",
      " |      \n",
      " |      # Later, in a different process/environment...\n",
      " |      reloaded_artifact = tf.saved_model.load(\"path/to/location\")\n",
      " |      predictions = reloaded_artifact.serve(input_data)\n",
      " |      ```\n",
      " |      \n",
      " |      If you would like to customize your serving endpoints, you can\n",
      " |      use the lower-level `keras.export.ExportArchive` class. The\n",
      " |      `export()` method relies on `ExportArchive` internally.\n",
      " |  \n",
      " |  get_layer(self, name=None, index=None)\n",
      " |      Retrieves a layer based on either its name (unique) or index.\n",
      " |      \n",
      " |      If `name` and `index` are both provided, `index` will take precedence.\n",
      " |      Indices are based on order of horizontal graph traversal (bottom-up).\n",
      " |      \n",
      " |      Args:\n",
      " |          name: String, name of layer.\n",
      " |          index: Integer, index of layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  get_state_tree(self, value_format='backend_tensor')\n",
      " |      Retrieves tree-like structure of model variables.\n",
      " |      \n",
      " |      This method allows retrieval of different model variables (trainable,\n",
      " |      non-trainable, optimizer, and metrics). The variables are returned in a\n",
      " |      nested dictionary format, where the keys correspond to the variable\n",
      " |      names and the values are the nested representations of the variables.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict: A dictionary containing the nested representations of the\n",
      " |              requested variables. The keys are the variable names, and the\n",
      " |              values are the corresponding nested dictionaries.\n",
      " |          value_format: One of `\"backend_tensor\"`, `\"numpy_array\"`.\n",
      " |              The kind of array to return as the leaves of the nested\n",
      " |                  state tree.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      model = keras.Sequential([\n",
      " |          keras.Input(shape=(1,), name=\"my_input\"),\n",
      " |          keras.layers.Dense(1, activation=\"sigmoid\", name=\"my_dense\"),\n",
      " |      ], name=\"my_sequential\")\n",
      " |      model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
      " |      model.fit(np.array([[1.0]]), np.array([[1.0]]))\n",
      " |      state_tree = model.get_state_tree()\n",
      " |      ```\n",
      " |      \n",
      " |      The `state_tree` dictionary returned looks like:\n",
      " |      \n",
      " |      ```\n",
      " |      {\n",
      " |          'metrics_variables': {\n",
      " |              'loss': {\n",
      " |                  'count': ...,\n",
      " |                  'total': ...,\n",
      " |              },\n",
      " |              'mean_absolute_error': {\n",
      " |                  'count': ...,\n",
      " |                  'total': ...,\n",
      " |              }\n",
      " |          },\n",
      " |          'trainable_variables': {\n",
      " |              'my_sequential': {\n",
      " |                  'my_dense': {\n",
      " |                      'bias': ...,\n",
      " |                      'kernel': ...,\n",
      " |                  }\n",
      " |              }\n",
      " |          },\n",
      " |          'non_trainable_variables': {},\n",
      " |          'optimizer_variables': {\n",
      " |              'adam': {\n",
      " |                      'iteration': ...,\n",
      " |                      'learning_rate': ...,\n",
      " |                      'my_sequential_my_dense_bias_momentum': ...,\n",
      " |                      'my_sequential_my_dense_bias_velocity': ...,\n",
      " |                      'my_sequential_my_dense_kernel_momentum': ...,\n",
      " |                      'my_sequential_my_dense_kernel_velocity': ...,\n",
      " |                  }\n",
      " |              }\n",
      " |          }\n",
      " |      }\n",
      " |      ```\n",
      " |  \n",
      " |  load_weights(self, filepath, skip_mismatch=False, **kwargs)\n",
      " |      Load weights from a file saved via `save_weights()`.\n",
      " |      \n",
      " |      Weights are loaded based on the network's\n",
      " |      topology. This means the architecture should be the same as when the\n",
      " |      weights were saved. Note that layers that don't have weights are not\n",
      " |      taken into account in the topological ordering, so adding or removing\n",
      " |      layers is fine as long as they don't have weights.\n",
      " |      \n",
      " |      **Partial weight loading**\n",
      " |      \n",
      " |      If you have modified your model, for instance by adding a new layer\n",
      " |      (with weights) or by changing the shape of the weights of a layer,\n",
      " |      you can choose to ignore errors and continue loading\n",
      " |      by setting `skip_mismatch=True`. In this case any layer with\n",
      " |      mismatching weights will be skipped. A warning will be displayed\n",
      " |      for each skipped layer.\n",
      " |      \n",
      " |      Args:\n",
      " |          filepath: String, path to the weights file to load.\n",
      " |              It can either be a `.weights.h5` file\n",
      " |              or a legacy `.h5` weights file.\n",
      " |          skip_mismatch: Boolean, whether to skip loading of layers where\n",
      " |              there is a mismatch in the number of weights, or a mismatch in\n",
      " |              the shape of the weights.\n",
      " |  \n",
      " |  quantize(self, mode, **kwargs)\n",
      " |      Quantize the weights of the model.\n",
      " |      \n",
      " |      Note that the model must be built first before calling this method.\n",
      " |      `quantize` will recursively call `quantize(mode)` in all layers and\n",
      " |      will be skipped if the layer doesn't implement the function.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode: The mode of the quantization. Only 'int8' is supported at this\n",
      " |              time.\n",
      " |  \n",
      " |  save(self, filepath, overwrite=True, zipped=None, **kwargs)\n",
      " |      Saves a model as a `.keras` file.\n",
      " |      \n",
      " |      Args:\n",
      " |          filepath: `str` or `pathlib.Path` object.\n",
      " |              The path where to save the model. Must end in `.keras`\n",
      " |              (unless saving the model as an unzipped directory\n",
      " |              via `zipped=False`).\n",
      " |          overwrite: Whether we should overwrite any existing model at\n",
      " |              the target location, or instead ask the user via\n",
      " |              an interactive prompt.\n",
      " |          zipped: Whether to save the model as a zipped `.keras`\n",
      " |              archive (default when saving locally), or as an\n",
      " |              unzipped directory (default when saving on the\n",
      " |              Hugging Face Hub).\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      model = keras.Sequential(\n",
      " |          [\n",
      " |              keras.layers.Dense(5, input_shape=(3,)),\n",
      " |              keras.layers.Softmax(),\n",
      " |          ],\n",
      " |      )\n",
      " |      model.save(\"model.keras\")\n",
      " |      loaded_model = keras.saving.load_model(\"model.keras\")\n",
      " |      x = keras.random.uniform((10, 3))\n",
      " |      assert np.allclose(model.predict(x), loaded_model.predict(x))\n",
      " |      ```\n",
      " |      \n",
      " |      Note that `model.save()` is an alias for `keras.saving.save_model()`.\n",
      " |      \n",
      " |      The saved `.keras` file contains:\n",
      " |      \n",
      " |      - The model's configuration (architecture)\n",
      " |      - The model's weights\n",
      " |      - The model's optimizer's state (if any)\n",
      " |      \n",
      " |      Thus models can be reinstantiated in the exact same state.\n",
      " |  \n",
      " |  save_weights(self, filepath, overwrite=True)\n",
      " |      Saves all layer weights to a `.weights.h5` file.\n",
      " |      \n",
      " |      Args:\n",
      " |          filepath: `str` or `pathlib.Path` object.\n",
      " |              Path where to save the model. Must end in `.weights.h5`.\n",
      " |          overwrite: Whether we should overwrite any existing model\n",
      " |              at the target location, or instead ask the user\n",
      " |              via an interactive prompt.\n",
      " |  \n",
      " |  set_state_tree(self, state_tree)\n",
      " |      Assigns values to variables of the model.\n",
      " |      \n",
      " |      This method takes a dictionary of nested variable values, which\n",
      " |      represents the state tree of the model, and assigns them to the\n",
      " |      corresponding variables of the model. The dictionary keys represent the\n",
      " |      variable names (e.g., `'trainable_variables'`, `'optimizer_variables'`),\n",
      " |      and the values are nested dictionaries containing the variable\n",
      " |      paths and their corresponding values.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_tree: A dictionary representing the state tree of the model.\n",
      " |              The keys are the variable names, and the values are nested\n",
      " |              dictionaries representing the variable paths and their values.\n",
      " |  \n",
      " |  summary(self, line_length=None, positions=None, print_fn=None, expand_nested=False, show_trainable=False, layer_range=None)\n",
      " |      Prints a string summary of the network.\n",
      " |      \n",
      " |      Args:\n",
      " |          line_length: Total length of printed lines\n",
      " |              (e.g. set this to adapt the display to different\n",
      " |              terminal window sizes).\n",
      " |          positions: Relative or absolute positions of log elements\n",
      " |              in each line. If not provided, becomes\n",
      " |              `[0.3, 0.6, 0.70, 1.]`. Defaults to `None`.\n",
      " |          print_fn: Print function to use. By default, prints to `stdout`.\n",
      " |              If `stdout` doesn't work in your environment, change to `print`.\n",
      " |              It will be called on each line of the summary.\n",
      " |              You can set it to a custom function\n",
      " |              in order to capture the string summary.\n",
      " |          expand_nested: Whether to expand the nested models.\n",
      " |              Defaults to `False`.\n",
      " |          show_trainable: Whether to show if a layer is trainable.\n",
      " |              Defaults to `False`.\n",
      " |          layer_range: a list or tuple of 2 strings,\n",
      " |              which is the starting layer name and ending layer name\n",
      " |              (both inclusive) indicating the range of layers to be printed\n",
      " |              in summary. It also accepts regex patterns instead of exact\n",
      " |              names. In this case, the start predicate will be\n",
      " |              the first element that matches `layer_range[0]`\n",
      " |              and the end predicate will be the last element\n",
      " |              that matches `layer_range[1]`.\n",
      " |              By default `None` considers all layers of the model.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if `summary()` is called before the model is built.\n",
      " |  \n",
      " |  to_json(self, **kwargs)\n",
      " |      Returns a JSON string containing the network configuration.\n",
      " |      \n",
      " |      To load a network from a JSON save file, use\n",
      " |      `keras.models.model_from_json(json_string, custom_objects={...})`.\n",
      " |      \n",
      " |      Args:\n",
      " |          **kwargs: Additional keyword arguments to be passed to\n",
      " |              `json.dumps()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON string.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.backend.tensorflow.trainer.TensorFlowTrainer:\n",
      " |  \n",
      " |  compiled_loss(self, y, y_pred, sample_weight=None, regularization_losses=None)\n",
      " |  \n",
      " |  evaluate(self, x=None, y=None, batch_size=None, verbose='auto', sample_weight=None, steps=None, callbacks=None, return_dict=False, **kwargs)\n",
      " |      Returns the loss value & metrics values for the model in test mode.\n",
      " |      \n",
      " |      Computation is done in batches (see the `batch_size` arg.)\n",
      " |      \n",
      " |      Args:\n",
      " |          x: Input data. It could be:\n",
      " |              - A NumPy array (or array-like), or a list of arrays\n",
      " |                  (in case the model has multiple inputs).\n",
      " |              - A tensor, or a list of tensors\n",
      " |                  (in case the model has multiple inputs).\n",
      " |              - A dict mapping input names to the corresponding array/tensors,\n",
      " |                  if the model has named inputs.\n",
      " |              - A `tf.data.Dataset`. Should return a tuple\n",
      " |                  of either `(inputs, targets)` or\n",
      " |                  `(inputs, targets, sample_weights)`.\n",
      " |              - A generator or `keras.utils.PyDataset` returning\n",
      " |                  `(inputs, targets)` or `(inputs, targets, sample_weights)`.\n",
      " |          y: Target data. Like the input data `x`, it could be either NumPy\n",
      " |              array(s) or backend-native tensor(s).\n",
      " |              If `x` is a `tf.data.Dataset` or `keras.utils.PyDataset`\n",
      " |              instance, `y` should not be specified\n",
      " |              (since targets will be obtained from the iterator/dataset).\n",
      " |          batch_size: Integer or `None`. Number of samples per batch of\n",
      " |              computation. If unspecified, `batch_size` will default to 32. Do\n",
      " |              not specify the `batch_size` if your data is in the form of a\n",
      " |              dataset, generators, or `keras.utils.PyDataset` instances\n",
      " |              (since they generate batches).\n",
      " |          verbose: `\"auto\"`, 0, 1, or 2. Verbosity mode.\n",
      " |              0 = silent, 1 = progress bar, 2 = single line.\n",
      " |              `\"auto\"` becomes 1 for most cases.\n",
      " |              Note that the progress bar is not\n",
      " |              particularly useful when logged to a file, so `verbose=2` is\n",
      " |              recommended when not running interactively\n",
      " |              (e.g. in a production environment). Defaults to `\"auto\"`.\n",
      " |          sample_weight: Optional NumPy array of weights for the test samples,\n",
      " |              used for weighting the loss function. You can either pass a flat\n",
      " |              (1D) NumPy array with the same length as the input samples\n",
      " |              (1:1 mapping between weights and samples), or in the case of\n",
      " |              temporal data, you can pass a 2D array with shape `(samples,\n",
      " |              sequence_length)`, to apply a different weight to every\n",
      " |              timestep of every sample. This argument is not supported when\n",
      " |              `x` is a dataset, instead pass sample weights as the third\n",
      " |              element of `x`.\n",
      " |          steps: Integer or `None`. Total number of steps (batches of samples)\n",
      " |              before declaring the evaluation round finished. Ignored with the\n",
      " |              default value of `None`. If `x` is a `tf.data.Dataset` and\n",
      " |              `steps` is `None`, evaluation will run until the dataset\n",
      " |              is exhausted.\n",
      " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
      " |              List of callbacks to apply during evaluation.\n",
      " |          return_dict: If `True`, loss and metric results are returned as a\n",
      " |              dict, with each key being the name of the metric.\n",
      " |              If `False`, they are returned as a list.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Scalar test loss (if the model has a single output and no metrics)\n",
      " |          or list of scalars (if the model has multiple outputs\n",
      " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |  \n",
      " |  fit(self, x=None, y=None, batch_size=None, epochs=1, verbose='auto', callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, validation_freq=1)\n",
      " |      Trains the model for a fixed number of epochs (dataset iterations).\n",
      " |      \n",
      " |      Args:\n",
      " |          x: Input data. It could be:\n",
      " |              - A NumPy array (or array-like), or a list of arrays\n",
      " |              (in case the model has multiple inputs).\n",
      " |              - A tensor, or a list of tensors\n",
      " |              (in case the model has multiple inputs).\n",
      " |              - A dict mapping input names to the corresponding array/tensors,\n",
      " |              if the model has named inputs.\n",
      " |              - A `tf.data.Dataset`. Should return a tuple\n",
      " |              of either `(inputs, targets)` or\n",
      " |              `(inputs, targets, sample_weights)`.\n",
      " |              - A `keras.utils.PyDataset` returning `(inputs,\n",
      " |              targets)` or `(inputs, targets, sample_weights)`.\n",
      " |          y: Target data. Like the input data `x`,\n",
      " |              it could be either NumPy array(s) or backend-native tensor(s).\n",
      " |              If `x` is a dataset, generator,\n",
      " |              or `keras.utils.PyDataset` instance, `y` should\n",
      " |              not be specified (since targets will be obtained from `x`).\n",
      " |          batch_size: Integer or `None`.\n",
      " |              Number of samples per gradient update.\n",
      " |              If unspecified, `batch_size` will default to 32.\n",
      " |              Do not specify the `batch_size` if your data is in the\n",
      " |              form of datasets, generators, or `keras.utils.PyDataset`\n",
      " |              instances (since they generate batches).\n",
      " |          epochs: Integer. Number of epochs to train the model.\n",
      " |              An epoch is an iteration over the entire `x` and `y`\n",
      " |              data provided\n",
      " |              (unless the `steps_per_epoch` flag is set to\n",
      " |              something other than None).\n",
      " |              Note that in conjunction with `initial_epoch`,\n",
      " |              `epochs` is to be understood as \"final epoch\".\n",
      " |              The model is not trained for a number of iterations\n",
      " |              given by `epochs`, but merely until the epoch\n",
      " |              of index `epochs` is reached.\n",
      " |          verbose: `\"auto\"`, 0, 1, or 2. Verbosity mode.\n",
      " |              0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      " |              \"auto\" becomes 1 for most cases.\n",
      " |              Note that the progress bar is not\n",
      " |              particularly useful when logged to a file,\n",
      " |              so `verbose=2` is recommended when not running interactively\n",
      " |              (e.g., in a production environment). Defaults to `\"auto\"`.\n",
      " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
      " |              List of callbacks to apply during training.\n",
      " |              See `keras.callbacks`. Note\n",
      " |              `keras.callbacks.ProgbarLogger` and\n",
      " |              `keras.callbacks.History` callbacks are created\n",
      " |              automatically and need not be passed to `model.fit()`.\n",
      " |              `keras.callbacks.ProgbarLogger` is created\n",
      " |              or not based on the `verbose` argument in `model.fit()`.\n",
      " |          validation_split: Float between 0 and 1.\n",
      " |              Fraction of the training data to be used as validation data.\n",
      " |              The model will set apart this fraction of the training data,\n",
      " |              will not train on it, and will evaluate\n",
      " |              the loss and any model metrics\n",
      " |              on this data at the end of each epoch.\n",
      " |              The validation data is selected from the last samples\n",
      " |              in the `x` and `y` data provided, before shuffling. This\n",
      " |              argument is not supported when `x` is a dataset, generator or\n",
      " |              `keras.utils.PyDataset` instance.\n",
      " |              If both `validation_data` and `validation_split` are provided,\n",
      " |              `validation_data` will override `validation_split`.\n",
      " |          validation_data: Data on which to evaluate\n",
      " |              the loss and any model metrics at the end of each epoch.\n",
      " |              The model will not be trained on this data. Thus, note the fact\n",
      " |              that the validation loss of data provided using\n",
      " |              `validation_split` or `validation_data` is not affected by\n",
      " |              regularization layers like noise and dropout.\n",
      " |              `validation_data` will override `validation_split`.\n",
      " |              It could be:\n",
      " |              - A tuple `(x_val, y_val)` of NumPy arrays or tensors.\n",
      " |              - A tuple `(x_val, y_val, val_sample_weights)` of NumPy\n",
      " |              arrays.\n",
      " |              - A `tf.data.Dataset`.\n",
      " |              - A Python generator or `keras.utils.PyDataset` returning\n",
      " |              `(inputs, targets)` or `(inputs, targets, sample_weights)`.\n",
      " |          shuffle: Boolean, whether to shuffle the training data\n",
      " |              before each epoch. This argument is\n",
      " |              ignored when `x` is a generator or a `tf.data.Dataset`.\n",
      " |          class_weight: Optional dictionary mapping class indices (integers)\n",
      " |              to a weight (float) value, used for weighting the loss function\n",
      " |              (during training only).\n",
      " |              This can be useful to tell the model to\n",
      " |              \"pay more attention\" to samples from\n",
      " |              an under-represented class. When `class_weight` is specified\n",
      " |              and targets have a rank of 2 or greater, either `y` must be\n",
      " |              one-hot encoded, or an explicit final dimension of `1` must\n",
      " |              be included for sparse class labels.\n",
      " |          sample_weight: Optional NumPy array of weights for\n",
      " |              the training samples, used for weighting the loss function\n",
      " |              (during training only). You can either pass a flat (1D)\n",
      " |              NumPy array with the same length as the input samples\n",
      " |              (1:1 mapping between weights and samples),\n",
      " |              or in the case of temporal data,\n",
      " |              you can pass a 2D array with shape\n",
      " |              `(samples, sequence_length)`,\n",
      " |              to apply a different weight to every timestep of every sample.\n",
      " |              This argument is not supported when `x` is a dataset, generator,\n",
      " |              or `keras.utils.PyDataset` instance, instead provide the\n",
      " |              sample_weights as the third element of `x`.\n",
      " |              Note that sample weighting does not apply to metrics specified\n",
      " |              via the `metrics` argument in `compile()`. To apply sample\n",
      " |              weighting to your metrics, you can specify them via the\n",
      " |              `weighted_metrics` in `compile()` instead.\n",
      " |          initial_epoch: Integer.\n",
      " |              Epoch at which to start training\n",
      " |              (useful for resuming a previous training run).\n",
      " |          steps_per_epoch: Integer or `None`.\n",
      " |              Total number of steps (batches of samples)\n",
      " |              before declaring one epoch finished and starting the\n",
      " |              next epoch. When training with input tensors such as\n",
      " |              backend-native tensors, the default `None` is equal to\n",
      " |              the number of samples in your dataset divided by\n",
      " |              the batch size, or 1 if that cannot be determined. If `x` is a\n",
      " |              `tf.data.Dataset`, and `steps_per_epoch`\n",
      " |              is `None`, the epoch will run until the input dataset is\n",
      " |              exhausted.  When passing an infinitely repeating dataset, you\n",
      " |              must specify the `steps_per_epoch` argument. If\n",
      " |              `steps_per_epoch=-1` the training will run indefinitely with an\n",
      " |              infinitely repeating dataset.\n",
      " |          validation_steps: Only relevant if `validation_data` is provided.\n",
      " |              Total number of steps (batches of\n",
      " |              samples) to draw before stopping when performing validation\n",
      " |              at the end of every epoch. If `validation_steps` is `None`,\n",
      " |              validation will run until the `validation_data` dataset is\n",
      " |              exhausted. In the case of an infinitely repeated dataset, it\n",
      " |              will run into an infinite loop. If `validation_steps` is\n",
      " |              specified and only part of the dataset will be consumed, the\n",
      " |              evaluation will start from the beginning of the dataset at each\n",
      " |              epoch. This ensures that the same validation samples are used\n",
      " |              every time.\n",
      " |          validation_batch_size: Integer or `None`.\n",
      " |              Number of samples per validation batch.\n",
      " |              If unspecified, will default to `batch_size`.\n",
      " |              Do not specify the `validation_batch_size` if your data is in\n",
      " |              the form of datasets or `keras.utils.PyDataset`\n",
      " |              instances (since they generate batches).\n",
      " |          validation_freq: Only relevant if validation data is provided.\n",
      " |              Specifies how many training epochs to run\n",
      " |              before a new validation run is performed,\n",
      " |              e.g. `validation_freq=2` runs validation every 2 epochs.\n",
      " |      \n",
      " |      Unpacking behavior for iterator-like inputs:\n",
      " |          A common pattern is to pass an iterator like object such as a\n",
      " |          `tf.data.Dataset` or a `keras.utils.PyDataset` to `fit()`,\n",
      " |          which will in fact yield not only features (`x`)\n",
      " |          but optionally targets (`y`) and sample weights (`sample_weight`).\n",
      " |          Keras requires that the output of such iterator-likes be\n",
      " |          unambiguous. The iterator should return a tuple\n",
      " |          of length 1, 2, or 3, where the optional second and third elements\n",
      " |          will be used for `y` and `sample_weight` respectively.\n",
      " |          Any other type provided will be wrapped in\n",
      " |          a length-one tuple, effectively treating everything as `x`. When\n",
      " |          yielding dicts, they should still adhere to the top-level tuple\n",
      " |          structure,\n",
      " |          e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n",
      " |          features, targets, and weights from the keys of a single dict.\n",
      " |          A notable unsupported data type is the `namedtuple`. The reason is\n",
      " |          that it behaves like both an ordered datatype (tuple) and a mapping\n",
      " |          datatype (dict). So given a namedtuple of the form:\n",
      " |          `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n",
      " |          it is ambiguous whether to reverse the order of the elements when\n",
      " |          interpreting the value. Even worse is a tuple of the form:\n",
      " |          `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n",
      " |          where it is unclear if the tuple was intended to be unpacked\n",
      " |          into `x`, `y`, and `sample_weight` or passed through\n",
      " |          as a single element to `x`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A `History` object. Its `History.history` attribute is\n",
      " |          a record of training loss values and metrics values\n",
      " |          at successive epochs, as well as validation loss values\n",
      " |          and validation metrics values (if applicable).\n",
      " |  \n",
      " |  loss(self, y, y_pred, sample_weight=None)\n",
      " |  \n",
      " |  make_predict_function(self, force=False)\n",
      " |  \n",
      " |  make_test_function(self, force=False)\n",
      " |  \n",
      " |  make_train_function(self, force=False)\n",
      " |  \n",
      " |  predict(self, x, batch_size=None, verbose='auto', steps=None, callbacks=None)\n",
      " |      Generates output predictions for the input samples.\n",
      " |      \n",
      " |      Computation is done in batches. This method is designed for batch\n",
      " |      processing of large numbers of inputs. It is not intended for use inside\n",
      " |      of loops that iterate over your data and process small numbers of inputs\n",
      " |      at a time.\n",
      " |      \n",
      " |      For small numbers of inputs that fit in one batch,\n",
      " |      directly use `__call__()` for faster execution, e.g.,\n",
      " |      `model(x)`, or `model(x, training=False)` if you have layers such as\n",
      " |      `BatchNormalization` that behave differently during\n",
      " |      inference.\n",
      " |      \n",
      " |      Note: See [this FAQ entry](\n",
      " |      https://keras.io/getting_started/faq/#whats-the-difference-between-model-methods-predict-and-call)\n",
      " |      for more details about the difference between `Model` methods\n",
      " |      `predict()` and `__call__()`.\n",
      " |      \n",
      " |      Args:\n",
      " |          x: Input samples. It could be:\n",
      " |              - A NumPy array (or array-like), or a list of arrays\n",
      " |                  (in case the model has multiple inputs).\n",
      " |              - A tensor, or a list of tensors\n",
      " |                  (in case the model has multiple inputs).\n",
      " |              - A `tf.data.Dataset`.\n",
      " |              - A `keras.utils.PyDataset` instance.\n",
      " |          batch_size: Integer or `None`.\n",
      " |              Number of samples per batch.\n",
      " |              If unspecified, `batch_size` will default to 32.\n",
      " |              Do not specify the `batch_size` if your data is in the\n",
      " |              form of dataset, generators, or `keras.utils.PyDataset`\n",
      " |              instances (since they generate batches).\n",
      " |          verbose: `\"auto\"`, 0, 1, or 2. Verbosity mode.\n",
      " |              0 = silent, 1 = progress bar, 2 = single line.\n",
      " |              `\"auto\"` becomes 1 for most cases. Note that the progress bar\n",
      " |              is not particularly useful when logged to a file,\n",
      " |              so `verbose=2` is recommended when not running interactively\n",
      " |              (e.g. in a production environment). Defaults to `\"auto\"`.\n",
      " |          steps: Total number of steps (batches of samples)\n",
      " |              before declaring the prediction round finished.\n",
      " |              Ignored with the default value of `None`.\n",
      " |              If `x` is a `tf.data.Dataset` and `steps` is `None`,\n",
      " |              `predict()` will run until the input dataset is exhausted.\n",
      " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
      " |              List of callbacks to apply during prediction.\n",
      " |      \n",
      " |      Returns:\n",
      " |          NumPy array(s) of predictions.\n",
      " |  \n",
      " |  predict_on_batch(self, x)\n",
      " |      Returns predictions for a single batch of samples.\n",
      " |      \n",
      " |      Args:\n",
      " |          x: Input data. It must be array-like.\n",
      " |      \n",
      " |      Returns:\n",
      " |          NumPy array(s) of predictions.\n",
      " |  \n",
      " |  predict_step(self, data)\n",
      " |  \n",
      " |  test_on_batch(self, x, y=None, sample_weight=None, return_dict=False)\n",
      " |      Test the model on a single batch of samples.\n",
      " |      \n",
      " |      Args:\n",
      " |          x: Input data. Must be array-like.\n",
      " |          y: Target data. Must be array-like.\n",
      " |          sample_weight: Optional array of the same length as x, containing\n",
      " |              weights to apply to the model's loss for each sample.\n",
      " |              In the case of temporal data, you can pass a 2D array\n",
      " |              with shape `(samples, sequence_length)`, to apply a different\n",
      " |              weight to every timestep of every sample.\n",
      " |          return_dict: If `True`, loss and metric results are returned as a\n",
      " |              dict, with each key being the name of the metric. If `False`,\n",
      " |              they are returned as a list.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A scalar loss value (when no metrics and `return_dict=False`),\n",
      " |          a list of loss and metric values\n",
      " |          (if there are metrics and `return_dict=False`), or a dict of\n",
      " |          metric and loss values (if `return_dict=True`).\n",
      " |  \n",
      " |  test_step(self, data)\n",
      " |  \n",
      " |  train_on_batch(self, x, y=None, sample_weight=None, class_weight=None, return_dict=False)\n",
      " |      Runs a single gradient update on a single batch of data.\n",
      " |      \n",
      " |      Args:\n",
      " |          x: Input data. Must be array-like.\n",
      " |          y: Target data. Must be array-like.\n",
      " |          sample_weight: Optional array of the same length as x, containing\n",
      " |              weights to apply to the model's loss for each sample.\n",
      " |              In the case of temporal data, you can pass a 2D array\n",
      " |              with shape `(samples, sequence_length)`, to apply a different\n",
      " |              weight to every timestep of every sample.\n",
      " |          class_weight: Optional dictionary mapping class indices (integers)\n",
      " |              to a weight (float) to apply to the model's loss for the samples\n",
      " |              from this class during training. This can be useful to tell the\n",
      " |              model to \"pay more attention\" to samples from an\n",
      " |              under-represented class. When `class_weight` is specified\n",
      " |              and targets have a rank of 2 or greater, either `y` must\n",
      " |              be one-hot encoded, or an explicit final dimension of 1\n",
      " |              must be included for sparse class labels.\n",
      " |          return_dict: If `True`, loss and metric results are returned as a\n",
      " |              dict, with each key being the name of the metric. If `False`,\n",
      " |              they are returned as a list.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A scalar loss value (when no metrics and `return_dict=False`),\n",
      " |          a list of loss and metric values\n",
      " |          (if there are metrics and `return_dict=False`), or a dict of\n",
      " |          metric and loss values (if `return_dict=True`).\n",
      " |  \n",
      " |  train_step(self, data)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from keras.src.backend.tensorflow.trainer.TensorFlowTrainer:\n",
      " |  \n",
      " |  compiled_metrics\n",
      " |  \n",
      " |  distribute_strategy\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.src.backend.tensorflow.trainer.TensorFlowTrainer:\n",
      " |  \n",
      " |  distribute_reduction_method\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.trainers.trainer.Trainer:\n",
      " |  \n",
      " |  compile(self, optimizer='rmsprop', loss=None, loss_weights=None, metrics=None, weighted_metrics=None, run_eagerly=False, steps_per_execution=1, jit_compile='auto', auto_scale_loss=True)\n",
      " |      Configures the model for training.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      model.compile(\n",
      " |          optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
      " |          loss=keras.losses.BinaryCrossentropy(),\n",
      " |          metrics=[\n",
      " |              keras.metrics.BinaryAccuracy(),\n",
      " |              keras.metrics.FalseNegatives(),\n",
      " |          ],\n",
      " |      )\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          optimizer: String (name of optimizer) or optimizer instance. See\n",
      " |              `keras.optimizers`.\n",
      " |          loss: Loss function. May be a string (name of loss function), or\n",
      " |              a `keras.losses.Loss` instance. See `keras.losses`. A\n",
      " |              loss function is any callable with the signature\n",
      " |              `loss = fn(y_true, y_pred)`, where `y_true` are the ground truth\n",
      " |              values, and `y_pred` are the model's predictions.\n",
      " |              `y_true` should have shape `(batch_size, d0, .. dN)`\n",
      " |              (except in the case of sparse loss functions such as\n",
      " |              sparse categorical crossentropy which expects integer arrays of\n",
      " |              shape `(batch_size, d0, .. dN-1)`).\n",
      " |              `y_pred` should have shape `(batch_size, d0, .. dN)`.\n",
      " |              The loss function should return a float tensor.\n",
      " |          loss_weights: Optional list or dictionary specifying scalar\n",
      " |              coefficients (Python floats) to weight the loss contributions of\n",
      " |              different model outputs. The loss value that will be minimized\n",
      " |              by the model will then be the *weighted sum* of all individual\n",
      " |              losses, weighted by the `loss_weights` coefficients.  If a list,\n",
      " |              it is expected to have a 1:1 mapping to the model's outputs. If\n",
      " |              a dict, it is expected to map output names (strings) to scalar\n",
      " |              coefficients.\n",
      " |          metrics: List of metrics to be evaluated by the model during\n",
      " |              training and testing. Each of this can be a string (name of a\n",
      " |              built-in function), function or a `keras.metrics.Metric`\n",
      " |              instance. See `keras.metrics`. Typically you will use\n",
      " |              `metrics=['accuracy']`. A function is any callable with the\n",
      " |              signature `result = fn(y_true, _pred)`. To specify different\n",
      " |              metrics for different outputs of a multi-output model, you could\n",
      " |              also pass a dictionary, such as\n",
      " |              `metrics={'a':'accuracy', 'b':['accuracy', 'mse']}`.\n",
      " |              You can also pass a list to specify a metric or a list of\n",
      " |              metrics for each output, such as\n",
      " |              `metrics=[['accuracy'], ['accuracy', 'mse']]`\n",
      " |              or `metrics=['accuracy', ['accuracy', 'mse']]`. When you pass\n",
      " |              the strings 'accuracy' or 'acc', we convert this to one of\n",
      " |              `keras.metrics.BinaryAccuracy`,\n",
      " |              `keras.metrics.CategoricalAccuracy`,\n",
      " |              `keras.metrics.SparseCategoricalAccuracy` based on the\n",
      " |              shapes of the targets and of the model output. A similar\n",
      " |              conversion is done for the strings `\"crossentropy\"`\n",
      " |              and `\"ce\"` as well.\n",
      " |              The metrics passed here are evaluated without sample weighting;\n",
      " |              if you would like sample weighting to apply, you can specify\n",
      " |              your metrics via the `weighted_metrics` argument instead.\n",
      " |          weighted_metrics: List of metrics to be evaluated and weighted by\n",
      " |              `sample_weight` or `class_weight` during training and testing.\n",
      " |          run_eagerly: Bool. If `True`, this model's forward pass\n",
      " |               will never be compiled. It is recommended to leave this\n",
      " |               as `False` when training (for best performance),\n",
      " |               and to set it to `True` when debugging.\n",
      " |          steps_per_execution: Int. The number of batches to run\n",
      " |              during each a single compiled function call. Running multiple\n",
      " |              batches inside a single compiled function call can\n",
      " |              greatly improve performance on TPUs or small models with a large\n",
      " |              Python overhead. At most, one full epoch will be run each\n",
      " |              execution. If a number larger than the size of the epoch is\n",
      " |              passed, the execution will be truncated to the size of the\n",
      " |              epoch. Note that if `steps_per_execution` is set to `N`,\n",
      " |              `Callback.on_batch_begin` and `Callback.on_batch_end` methods\n",
      " |              will only be called every `N` batches (i.e. before/after\n",
      " |              each compiled function execution).\n",
      " |              Not supported with the PyTorch backend.\n",
      " |          jit_compile: Bool or `\"auto\"`. Whether to use XLA compilation when\n",
      " |              compiling a model. For `jax` and `tensorflow` backends,\n",
      " |              `jit_compile=\"auto\"` enables XLA compilation if the model\n",
      " |              supports it, and disabled otherwise.\n",
      " |              For `torch` backend, `\"auto\"` will default to eager\n",
      " |              execution and `jit_compile=True` will run with `torch.compile`\n",
      " |              with the `\"inductor\"` backend.\n",
      " |          auto_scale_loss: Bool. If `True` and the model dtype policy is\n",
      " |              `\"mixed_float16\"`, the passed optimizer will be automatically\n",
      " |              wrapped in a `LossScaleOptimizer`, which will dynamically\n",
      " |              scale the loss to prevent underflow.\n",
      " |  \n",
      " |  compile_from_config(self, config)\n",
      " |      Compiles the model with the information given in config.\n",
      " |      \n",
      " |      This method uses the information in the config (optimizer, loss,\n",
      " |      metrics, etc.) to compile the model.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: Dict containing information for compiling the model.\n",
      " |  \n",
      " |  compute_loss(self, x=None, y=None, y_pred=None, sample_weight=None, training=True)\n",
      " |      Compute the total loss, validate it, and return it.\n",
      " |      \n",
      " |      Subclasses can optionally override this method to provide custom loss\n",
      " |      computation logic.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyModel(Model):\n",
      " |          def __init__(self, *args, **kwargs):\n",
      " |              super().__init__(*args, **kwargs)\n",
      " |              self.loss_tracker = metrics.Mean(name='loss')\n",
      " |      \n",
      " |          def compute_loss(self, x, y, y_pred, sample_weight, training=True):\n",
      " |              loss = ops.mean((y_pred - y) ** 2)\n",
      " |              loss += ops.sum(self.losses)\n",
      " |              self.loss_tracker.update_state(loss)\n",
      " |              return loss\n",
      " |      \n",
      " |          def reset_metrics(self):\n",
      " |              self.loss_tracker.reset_state()\n",
      " |      \n",
      " |          @property\n",
      " |          def metrics(self):\n",
      " |              return [self.loss_tracker]\n",
      " |      \n",
      " |      inputs = layers.Input(shape=(10,), name='my_input')\n",
      " |      outputs = layers.Dense(10)(inputs)\n",
      " |      model = MyModel(inputs, outputs)\n",
      " |      model.add_loss(ops.sum(outputs))\n",
      " |      \n",
      " |      optimizer = SGD()\n",
      " |      model.compile(optimizer, loss='mse', steps_per_execution=10)\n",
      " |      dataset = ...\n",
      " |      model.fit(dataset, epochs=2, steps_per_epoch=10)\n",
      " |      print(f\"Custom loss: {model.loss_tracker.result()}\")\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          x: Input data.\n",
      " |          y: Target data.\n",
      " |          y_pred: Predictions returned by the model (output of `model(x)`)\n",
      " |          sample_weight: Sample weights for weighting the loss function.\n",
      " |          training: Whether we are training or evaluating the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The total loss as a scalar tensor, or `None` if no loss results\n",
      " |          (which is the case when called by `Model.test_step`).\n",
      " |  \n",
      " |  compute_metrics(self, x, y, y_pred, sample_weight=None)\n",
      " |      Update metric states and collect all metrics to be returned.\n",
      " |      \n",
      " |      Subclasses can optionally override this method to provide custom metric\n",
      " |      updating and collection logic. Custom metrics are not passed in\n",
      " |      `compile()`, they can be created in `__init__` or `build`. They are\n",
      " |      automatically tracked and returned by `self.metrics`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyModel(Sequential):\n",
      " |          def __init__(self, *args, **kwargs):\n",
      " |              super().__init__(*args, **kwargs)\n",
      " |              self.custom_metric = MyMetric(name=\"custom_metric\")\n",
      " |      \n",
      " |          def compute_metrics(self, x, y, y_pred, sample_weight):\n",
      " |              # This super call updates metrics from `compile` and returns\n",
      " |              # results for all metrics listed in `self.metrics`.\n",
      " |              metric_results = super().compute_metrics(\n",
      " |                  x, y, y_pred, sample_weight)\n",
      " |      \n",
      " |              # `metric_results` contains the previous result for\n",
      " |              # `custom_metric`, this is where we update it.\n",
      " |              self.custom_metric.update_state(x, y, y_pred, sample_weight)\n",
      " |              metric_results['custom_metric'] = self.custom_metric.result()\n",
      " |              return metric_results\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          x: Input data.\n",
      " |          y: Target data.\n",
      " |          y_pred: Predictions returned by the model output of `model.call(x)`.\n",
      " |          sample_weight: Sample weights for weighting the loss function.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A `dict` containing values that will be passed to\n",
      " |          `keras.callbacks.CallbackList.on_train_batch_end()`. Typically,\n",
      " |          the values of the metrics listed in `self.metrics` are returned.\n",
      " |          Example: `{'loss': 0.2, 'accuracy': 0.7}`.\n",
      " |  \n",
      " |  get_compile_config(self)\n",
      " |      Returns a serialized config with information for compiling the model.\n",
      " |      \n",
      " |      This method returns a config dictionary containing all the information\n",
      " |      (optimizer, loss, metrics, etc.) with which the model was compiled.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dict containing information for compiling the model.\n",
      " |  \n",
      " |  get_metrics_result(self)\n",
      " |      Returns the model's metrics values as a dict.\n",
      " |      \n",
      " |      If any of the metric result is a dict (containing multiple metrics),\n",
      " |      each of them gets added to the top level returned dict of this method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A `dict` containing values of the metrics listed in `self.metrics`.\n",
      " |          Example: `{'loss': 0.2, 'accuracy': 0.7}`.\n",
      " |  \n",
      " |  reset_metrics(self)\n",
      " |  \n",
      " |  stateless_compute_loss(self, trainable_variables, non_trainable_variables, metrics_variables, x=None, y=None, y_pred=None, sample_weight=None, training=True)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from keras.src.trainers.trainer.Trainer:\n",
      " |  \n",
      " |  metrics\n",
      " |  \n",
      " |  metrics_names\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.src.trainers.trainer.Trainer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |  \n",
      " |  jit_compile\n",
      " |  \n",
      " |  run_eagerly\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.layers.layer.Layer:\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  add_loss(self, loss)\n",
      " |      Can be called inside of the `call()` method to add a scalar loss.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(Layer):\n",
      " |          ...\n",
      " |          def call(self, x):\n",
      " |              self.add_loss(ops.sum(x))\n",
      " |              return x\n",
      " |      ```\n",
      " |  \n",
      " |  add_metric(self, *args, **kwargs)\n",
      " |  \n",
      " |  add_variable(self, shape, initializer, dtype=None, trainable=True, autocast=True, regularizer=None, constraint=None, name=None)\n",
      " |      Add a weight variable to the layer.\n",
      " |      \n",
      " |      Alias of `add_weight()`.\n",
      " |  \n",
      " |  add_weight(self, shape=None, initializer=None, dtype=None, trainable=True, autocast=True, regularizer=None, constraint=None, aggregation='mean', name=None)\n",
      " |      Add a weight variable to the layer.\n",
      " |      \n",
      " |      Args:\n",
      " |          shape: Shape tuple for the variable. Must be fully-defined\n",
      " |              (no `None` entries). Defaults to `()` (scalar) if unspecified.\n",
      " |          initializer: Initializer object to use to populate the initial\n",
      " |              variable value, or string name of a built-in initializer\n",
      " |              (e.g. `\"random_normal\"`). If unspecified, defaults to\n",
      " |              `\"glorot_uniform\"` for floating-point variables and to `\"zeros\"`\n",
      " |              for all other types (e.g. int, bool).\n",
      " |          dtype: Dtype of the variable to create, e.g. `\"float32\"`. If\n",
      " |              unspecified, defaults to the layer's variable dtype\n",
      " |              (which itself defaults to `\"float32\"` if unspecified).\n",
      " |          trainable: Boolean, whether the variable should be trainable via\n",
      " |              backprop or whether its updates are managed manually. Defaults\n",
      " |              to `True`.\n",
      " |          autocast: Boolean, whether to autocast layers variables when\n",
      " |              accessing them. Defaults to `True`.\n",
      " |          regularizer: Regularizer object to call to apply penalty on the\n",
      " |              weight. These penalties are summed into the loss function\n",
      " |              during optimization. Defaults to `None`.\n",
      " |          constraint: Contrainst object to call on the variable after any\n",
      " |              optimizer update, or string name of a built-in constraint.\n",
      " |              Defaults to `None`.\n",
      " |          aggregation: String, one of `'mean'`, `'sum'`,\n",
      " |              `'only_first_replica'`. Annotates the variable with the type\n",
      " |              of multi-replica aggregation to be used for this variable\n",
      " |              when writing custom data parallel training loops.\n",
      " |          name: String name of the variable. Useful for debugging purposes.\n",
      " |  \n",
      " |  compute_mask(self, inputs, previous_mask)\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |  \n",
      " |  get_build_config(self)\n",
      " |      Returns a dictionary with the layer's input shape.\n",
      " |      \n",
      " |      This method returns a config dict that can be used by\n",
      " |      `build_from_config(config)` to create all states (e.g. Variables and\n",
      " |      Lookup tables) needed by the layer.\n",
      " |      \n",
      " |      By default, the config only contains the input shape that the layer\n",
      " |      was built with. If you're writing a custom layer that creates state in\n",
      " |      an unusual way, you should override this method to make sure this state\n",
      " |      is already created when Keras attempts to load its value upon model\n",
      " |      loading.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dict containing the input shape associated with the layer.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Return the values of `layer.weights` as a list of NumPy arrays.\n",
      " |  \n",
      " |  load_own_variables(self, store)\n",
      " |      Loads the state of the layer.\n",
      " |      \n",
      " |      You can override this method to take full control of how the state of\n",
      " |      the layer is loaded upon calling `keras.models.load_model()`.\n",
      " |      \n",
      " |      Args:\n",
      " |          store: Dict from which the state of the model will be loaded.\n",
      " |  \n",
      " |  quantized_build(self, input_shape, mode)\n",
      " |  \n",
      " |  quantized_call(self, *args, **kwargs)\n",
      " |  \n",
      " |  save_own_variables(self, store)\n",
      " |      Saves the state of the layer.\n",
      " |      \n",
      " |      You can override this method to take full control of how the state of\n",
      " |      the layer is saved upon calling `model.save()`.\n",
      " |      \n",
      " |      Args:\n",
      " |          store: Dict where the state of the model will be saved.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the values of `layer.weights` from a list of NumPy arrays.\n",
      " |  \n",
      " |  stateless_call(self, trainable_variables, non_trainable_variables, *args, return_losses=False, **kwargs)\n",
      " |      Call the layer without any side effects.\n",
      " |      \n",
      " |      Args:\n",
      " |          trainable_variables: List of trainable variables of the model.\n",
      " |          non_trainable_variables: List of non-trainable variables of the\n",
      " |              model.\n",
      " |          *args: Positional arguments to be passed to `call()`.\n",
      " |          return_losses: If `True`, `stateless_call()` will return the list of\n",
      " |              losses created during `call()` as part of its return values.\n",
      " |          **kwargs: Keyword arguments to be passed to `call()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tuple. By default, returns `(outputs, non_trainable_variables)`.\n",
      " |              If `return_losses = True`, then returns\n",
      " |              `(outputs, non_trainable_variables, losses)`.\n",
      " |      \n",
      " |      Note: `non_trainable_variables` include not only non-trainable weights\n",
      " |      such as `BatchNormalization` statistics, but also RNG seed state\n",
      " |      (if there are any random operations part of the layer, such as dropout),\n",
      " |      and `Metric` state (if there are any metrics attached to the layer).\n",
      " |      These are all elements of state of the layer.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      model = ...\n",
      " |      data = ...\n",
      " |      trainable_variables = model.trainable_variables\n",
      " |      non_trainable_variables = model.non_trainable_variables\n",
      " |      # Call the model with zero side effects\n",
      " |      outputs, non_trainable_variables = model.stateless_call(\n",
      " |          trainable_variables,\n",
      " |          non_trainable_variables,\n",
      " |          data,\n",
      " |      )\n",
      " |      # Attach the updated state to the model\n",
      " |      # (until you do this, the model is still in its pre-call state).\n",
      " |      for ref_var, value in zip(\n",
      " |          model.non_trainable_variables, non_trainable_variables\n",
      " |      ):\n",
      " |          ref_var.assign(value)\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from keras.src.layers.layer.Layer:\n",
      " |  \n",
      " |  compute_dtype\n",
      " |      The dtype of the computations performed by the layer.\n",
      " |  \n",
      " |  dtype\n",
      " |      Alias of `layer.variable_dtype`.\n",
      " |  \n",
      " |  losses\n",
      " |      List of scalar losses from `add_loss`, regularizers and sublayers.\n",
      " |  \n",
      " |  metrics_variables\n",
      " |      List of all metric variables.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |      List of all non-trainable layer state.\n",
      " |      \n",
      " |      This extends `layer.non_trainable_weights` to include all state used by\n",
      " |      the layer including state for metrics and `SeedGenerator`s.\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |      List of all non-trainable weight variables of the layer.\n",
      " |      \n",
      " |      These are the weights that should not be updated by the optimizer during\n",
      " |      training. Unlike, `layer.non_trainable_variables` this excludes metric\n",
      " |      state and random seeds.\n",
      " |  \n",
      " |  path\n",
      " |      The path of the layer.\n",
      " |      \n",
      " |      If the layer has not been built yet, it will be `None`.\n",
      " |  \n",
      " |  quantization_mode\n",
      " |      The quantization mode of this layer, `None` if not quantized.\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      List of all trainable layer state.\n",
      " |      \n",
      " |      This is equivalent to `layer.trainable_weights`.\n",
      " |  \n",
      " |  trainable_weights\n",
      " |      List of all trainable weight variables of the layer.\n",
      " |      \n",
      " |      These are the weights that get updated by the optimizer during training.\n",
      " |  \n",
      " |  variable_dtype\n",
      " |      The dtype of the state (weights) of the layer.\n",
      " |  \n",
      " |  variables\n",
      " |      List of all layer state, including random seeds.\n",
      " |      \n",
      " |      This extends `layer.weights` to include all state used by the layer\n",
      " |      including `SeedGenerator`s.\n",
      " |      \n",
      " |      Note that metrics variables are not included here, use\n",
      " |      `metrics_variables` to visit all the metric variables.\n",
      " |  \n",
      " |  weights\n",
      " |      List of all weight variables of the layer.\n",
      " |      \n",
      " |      Unlike, `layer.variables` this excludes metric state and random seeds.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.src.layers.layer.Layer:\n",
      " |  \n",
      " |  dtype_policy\n",
      " |  \n",
      " |  input_spec\n",
      " |  \n",
      " |  supports_masking\n",
      " |      Whether this layer supports computing a mask using `compute_mask`.\n",
      " |  \n",
      " |  trainable\n",
      " |      Settable boolean, whether this layer should be trainable or not.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.ops.operation.Operation:\n",
      " |  \n",
      " |  symbolic_call(self, *args, **kwargs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from keras.src.ops.operation.Operation:\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a symbolic operation.\n",
      " |      \n",
      " |      Only returns the tensor(s) corresponding to the *first time*\n",
      " |      the operation was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only returns the tensor(s) corresponding to the *first time*\n",
      " |      the operation was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output tensor or list of output tensors.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.saving.keras_saveable.KerasSaveable:\n",
      " |  \n",
      " |  __reduce__(self)\n",
      " |      __reduce__ is used to customize the behavior of `pickle.pickle()`.\n",
      " |      \n",
      " |      The method returns a tuple of two elements: a function, and a list of\n",
      " |      arguments to pass to that function.  In this case we just leverage the\n",
      " |      keras saving library.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Sequential)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d9ac1a",
   "metadata": {},
   "source": [
    "### Step 7: Fit and test the accuracy\n",
    "- Fit the model on training data with **epochs=20**\n",
    "- Evaluate the model with test data with **verbose=2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea826758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 653us/step - accuracy: 0.5450 - loss: 0.8438\n",
      "Epoch 2/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 635us/step - accuracy: 0.5929 - loss: 0.7167\n",
      "Epoch 3/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6420 - loss: 0.6489\n",
      "Epoch 4/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6608 - loss: 0.5917\n",
      "Epoch 5/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7338 - loss: 0.5036\n",
      "Epoch 6/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7666 - loss: 0.4804\n",
      "Epoch 7/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - accuracy: 0.7972 - loss: 0.4422\n",
      "Epoch 8/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - accuracy: 0.8275 - loss: 0.3957\n",
      "Epoch 9/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step - accuracy: 0.8151 - loss: 0.3970\n",
      "Epoch 10/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 778us/step - accuracy: 0.8539 - loss: 0.3531\n",
      "Epoch 11/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8692 - loss: 0.3156\n",
      "Epoch 12/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8736 - loss: 0.3135\n",
      "Epoch 13/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 991us/step - accuracy: 0.8833 - loss: 0.2786\n",
      "Epoch 14/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - accuracy: 0.8989 - loss: 0.2695\n",
      "Epoch 15/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630us/step - accuracy: 0.9137 - loss: 0.2440\n",
      "Epoch 16/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - accuracy: 0.9149 - loss: 0.2201\n",
      "Epoch 17/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - accuracy: 0.9274 - loss: 0.2076\n",
      "Epoch 18/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step - accuracy: 0.9234 - loss: 0.1967\n",
      "Epoch 19/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9592 - loss: 0.1570  \n",
      "Epoch 20/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - accuracy: 0.9578 - loss: 0.1547\n",
      "18/18 - 0s - 6ms/step - accuracy: 0.9727 - loss: 0.1391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1390683501958847, 0.9726775884628296]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20)\n",
    "model.evaluate(X_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ee1716",
   "metadata": {},
   "source": [
    "### Step 8: Add another hidden layer\n",
    "- Add another hidden layer in the model\n",
    "- Test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e079a7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(4,)))\n",
    "model.add(Dense(units=8, activation='relu'))\n",
    "model.add(Dense(units=4, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "015a1901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.3267 - loss: 0.9710  \n",
      "Epoch 2/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 963us/step - accuracy: 0.5260 - loss: 0.7868\n",
      "Epoch 3/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5397 - loss: 0.7166\n",
      "Epoch 4/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - accuracy: 0.5572 - loss: 0.6589\n",
      "Epoch 5/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5850 - loss: 0.6172\n",
      "Epoch 6/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 837us/step - accuracy: 0.6229 - loss: 0.5920\n",
      "Epoch 7/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 564us/step - accuracy: 0.6141 - loss: 0.5836\n",
      "Epoch 8/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step - accuracy: 0.6651 - loss: 0.5524\n",
      "Epoch 9/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - accuracy: 0.6826 - loss: 0.5188\n",
      "Epoch 10/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step - accuracy: 0.6923 - loss: 0.5137\n",
      "Epoch 11/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7307 - loss: 0.4747\n",
      "Epoch 12/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7774 - loss: 0.4461\n",
      "Epoch 13/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7946 - loss: 0.4373\n",
      "Epoch 14/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8458 - loss: 0.4024\n",
      "Epoch 15/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8674 - loss: 0.3564\n",
      "Epoch 16/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9239 - loss: 0.3279\n",
      "Epoch 17/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9489 - loss: 0.2937\n",
      "Epoch 18/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9634 - loss: 0.2698\n",
      "Epoch 19/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9681 - loss: 0.2461\n",
      "Epoch 20/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 632us/step - accuracy: 0.9802 - loss: 0.2164\n",
      "18/18 - 0s - 6ms/step - accuracy: 0.9745 - loss: 0.2027\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.20273055136203766, 0.9744991064071655]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20)\n",
    "model.evaluate(X_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3819949f",
   "metadata": {},
   "source": [
    "### Step 9: Add Dropout before Dense layer\n",
    "- Test how affects performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "247a6475",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(4,)))\n",
    "model.add(Dense(units=8, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=4, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8df1b3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.4117 - loss: 0.9952   \n",
      "Epoch 2/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4353 - loss: 0.8016\n",
      "Epoch 3/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4694 - loss: 0.8048\n",
      "Epoch 4/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4978 - loss: 0.7097\n",
      "Epoch 5/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5976 - loss: 0.6538\n",
      "Epoch 6/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 429us/step - accuracy: 0.6320 - loss: 0.6153\n",
      "Epoch 7/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 790us/step - accuracy: 0.7205 - loss: 0.5525\n",
      "Epoch 8/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - accuracy: 0.7400 - loss: 0.5357\n",
      "Epoch 9/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 899us/step - accuracy: 0.7896 - loss: 0.4909\n",
      "Epoch 10/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 775us/step - accuracy: 0.7792 - loss: 0.4896\n",
      "Epoch 11/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - accuracy: 0.8266 - loss: 0.4314\n",
      "Epoch 12/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - accuracy: 0.8467 - loss: 0.4300\n",
      "Epoch 13/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8716 - loss: 0.3922\n",
      "Epoch 14/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8852 - loss: 0.3813\n",
      "Epoch 15/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9083 - loss: 0.3454\n",
      "Epoch 16/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9065 - loss: 0.3489\n",
      "Epoch 17/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step - accuracy: 0.9208 - loss: 0.3350\n",
      "Epoch 18/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - accuracy: 0.9316 - loss: 0.3115\n",
      "Epoch 19/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - accuracy: 0.9254 - loss: 0.3178\n",
      "Epoch 20/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - accuracy: 0.9638 - loss: 0.2923\n",
      "18/18 - 0s - 6ms/step - accuracy: 0.9854 - loss: 0.2595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.25946715474128723, 0.9854280352592468]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20)\n",
    "model.evaluate(X_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf38f15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(4,)))\n",
    "model.add(Dense(units=8, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=4, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c0cb78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 558us/step - accuracy: 0.6681 - loss: 0.7912\n",
      "Epoch 2/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - accuracy: 0.7330 - loss: 0.6018\n",
      "Epoch 3/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - accuracy: 0.7523 - loss: 0.5213\n",
      "Epoch 4/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7856 - loss: 0.4120\n",
      "Epoch 5/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8273 - loss: 0.3374\n",
      "Epoch 6/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8618 - loss: 0.3078\n",
      "Epoch 7/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8404 - loss: 0.3009\n",
      "Epoch 8/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8874 - loss: 0.2665\n",
      "Epoch 9/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8745 - loss: 0.2856\n",
      "Epoch 10/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8750 - loss: 0.2609\n",
      "Epoch 11/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - accuracy: 0.8846 - loss: 0.2475\n",
      "Epoch 12/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 596us/step - accuracy: 0.8995 - loss: 0.2239\n",
      "Epoch 13/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step - accuracy: 0.9077 - loss: 0.1952\n",
      "Epoch 14/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9211 - loss: 0.2034\n",
      "Epoch 15/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - accuracy: 0.9370 - loss: 0.1618\n",
      "Epoch 16/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - accuracy: 0.9186 - loss: 0.1901\n",
      "Epoch 17/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - accuracy: 0.9213 - loss: 0.1908\n",
      "Epoch 18/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - accuracy: 0.9368 - loss: 0.1788\n",
      "Epoch 19/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - accuracy: 0.9278 - loss: 0.1757\n",
      "Epoch 20/20\n",
      "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 767us/step - accuracy: 0.9469 - loss: 0.1672\n",
      "18/18 - 0s - 6ms/step - accuracy: 0.9927 - loss: 0.0661\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06606898456811905, 0.9927140474319458]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20)\n",
    "model.evaluate(X_test, y_test, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
